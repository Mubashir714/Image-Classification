{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtkcwEv-oCsp"
      },
      "outputs": [],
      "source": [
        "%pip install tensorflow tensorflow-gpu opencv-python matplotlib,numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing Indispendable dependencies**\n",
        "\n",
        "\n",
        "*   Tensorflow\n",
        "*   Tensorflow-gpu\n",
        "*   Opencv-python\n",
        "*   Matplotlib\n",
        "*   Numpy\n"
      ],
      "metadata": {
        "id": "IGtZeJGA3ewv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNZxf-qCoCsr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np # type: ignore\n",
        "import keras # type: ignore\n",
        "from keras import layers # type: ignore\n",
        "from tensorflow import data as tf_data # type: ignore\n",
        "import matplotlib.pyplot as plt # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the install dependencies"
      ],
      "metadata": {
        "id": "nru_pbLS4mjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_folder = '/content/drive/My Drive/Dataset/PetImages'\n",
        "import os\n",
        "os.listdir(dataset_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "Syz-HpcykfFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect Colab with google drive to get the dataset."
      ],
      "metadata": {
        "id": "vd07YYUP40rr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IogjSNVoCsr"
      },
      "outputs": [],
      "source": [
        "corrupted_count = 0  # Counter for corrupted images\n",
        "\n",
        "# Ensure the dataset folder exists\n",
        "if not os.path.exists(dataset_folder):\n",
        "    print(f\"Dataset folder '{dataset_folder}' not found.\")\n",
        "else:\n",
        "    for category in (\"Cat\", \"Dog\"):\n",
        "        category_path = os.path.join(dataset_folder, category)\n",
        "        if not os.path.exists(category_path):\n",
        "            print(f\"Category folder '{category_path}' not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "            try:\n",
        "                with open(image_path, \"rb\") as image_file:\n",
        "                    is_valid_jpeg = b\"JFIF\" in image_file.peek(10)\n",
        "            except Exception as e:\n",
        "                # Handle unexpected errors during file access\n",
        "                print(f\"Error reading file {image_path}: {e}\")\n",
        "                is_valid_jpeg = False\n",
        "\n",
        "            if not is_valid_jpeg:\n",
        "                corrupted_count += 1\n",
        "                # Delete corrupted image\n",
        "                try:\n",
        "                    os.remove(image_path)\n",
        "                    print(f\"Deleted corrupted file: {image_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error deleting file {image_path}: {e}\")\n",
        "\n",
        "print(f\"Deleted {corrupted_count} corrupted images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the dataset by evicting:\n",
        "\n",
        "*   Corrupted images\n",
        "*   Damaged images\n",
        "*   Not in the required format"
      ],
      "metadata": {
        "id": "Kw88ilTv5JoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ic7LC_XoCss"
      },
      "outputs": [],
      "source": [
        "image_size = (180, 180)\n",
        "batch_size = 128\n",
        "\n",
        "train_ds, val_ds = keras.utils.image_dataset_from_directory(\n",
        "    dataset_folder,\n",
        "    validation_split=0.2,\n",
        "    subset=\"both\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WX4z59LZ5vSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "VEJoD_HksZve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visulizing the Images in the require format"
      ],
      "metadata": {
        "id": "NDERkRLU6BXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_layers = [\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "]\n",
        "\n",
        "\n",
        "def data_augmentation(images):\n",
        "    for layer in data_augmentation_layers:\n",
        "        images = layer(images)\n",
        "    return images"
      ],
      "metadata": {
        "id": "Rn9tWRdIsoyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (180, 180)\n",
        "num_channels = 3\n",
        "input_shape = (*image_size, num_channels)\n",
        "\n",
        "# Model inputs and preprocessing\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n"
      ],
      "metadata": {
        "id": "3vowDlpEssW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda img, label: (data_augmentation(img), label),\n",
        "    num_parallel_calls=tf_data.AUTOTUNE,\n",
        ")\n",
        "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
        "train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "G7AVTG2VsvTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Rescaling(1.0 / 255)(inputs)\n",
        "    x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for size in [256, 512, 728]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    if num_classes == 2:\n",
        "        units = 1\n",
        "    else:\n",
        "        units = num_classes\n",
        "\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    # We specify activation=None so as to return logits\n",
        "    outputs = layers.Dense(units, activation=None)(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model = make_model(input_shape=image_size + (3,), num_classes=2)\n",
        "keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "q4T1lqEes5EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
        "]\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(3e-4),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
        ")\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=val_ds,\n",
        ")"
      ],
      "metadata": {
        "id": "QCbNt720tGjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = keras.utils.load_img(\"/content/drive/My Drive/Datasets/PetImages/Cat/12499.jpg\", target_size=image_size)\n",
        "plt.imshow(img)\n",
        "\n",
        "img_array = keras.utils.img_to_array(img)\n",
        "img_array = keras.ops.expand_dims(img_array, 0)  # Create batch axis\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = float(keras.ops.sigmoid(predictions[0][0]))\n",
        "print(f\"This image is {100 * (1 - score):.2f}% cat and {100 * score:.2f}% dog.\")"
      ],
      "metadata": {
        "id": "CwHMdtiI6WwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Inferencing on new data"
      ],
      "metadata": {
        "id": "PEvnsQ6e7V-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Mubashir714/Image-Classification.git"
      ],
      "metadata": {
        "id": "uFiaBiEZ87Mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}